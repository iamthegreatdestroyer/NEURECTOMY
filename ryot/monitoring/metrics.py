"""
Prometheus Metrics for Ryot LLM Service
Production monitoring for inference requests, token generation, and model performance
"""

from prometheus_client import Counter, Histogram, Gauge, Info
from functools import wraps
import time
from typing import Callable, Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)


# ============================================================================
# Inference Request Metrics
# ============================================================================

inference_requests_total = Counter(
    'ryot_inference_requests_total',
    'Total inference requests to Ryot LLM',
    ['model', 'status'],  # status: success, error, timeout
    help='Count of inference requests by model and status'
)

inference_request_duration_seconds = Histogram(
    'ryot_inference_request_duration_seconds',
    'Inference request latency in seconds',
    ['model'],
    buckets=(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0),
    help='End-to-end inference latency (includes queue + compute + serialize)'
)

# Time to First Token (important metric for streaming)
inference_ttft_seconds = Histogram(
    'ryot_inference_ttft_seconds',
    'Time to First Token for streaming inference',
    ['model'],
    buckets=(0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5),
    help='Time from request start to first token generation (critical for UX)'
)

# Time between tokens (generation speed)
inference_inter_token_latency_seconds = Histogram(
    'ryot_inference_inter_token_latency_seconds',
    'Inter-token latency (time between consecutive tokens)',
    ['model'],
    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5),
    help='Latency between token generations (target: <50ms for smooth UX)'
)


# ============================================================================
# Token Generation Metrics
# ============================================================================

tokens_generated_total = Counter(
    'ryot_tokens_generated_total',
    'Total tokens generated by Ryot LLM',
    ['model'],
    help='Cumulative token count by model'
)

tokens_per_request = Histogram(
    'ryot_tokens_per_request',
    'Tokens generated per inference request',
    ['model'],
    buckets=(1, 5, 10, 25, 50, 100, 250, 500, 1000, 2000),
    help='Distribution of token counts per request'
)

tokens_per_second = Gauge(
    'ryot_tokens_per_second',
    'Current token generation rate (tokens/second)',
    ['model'],
    help='Real-time throughput metric'
)

average_tokens_per_minute = Gauge(
    'ryot_average_tokens_per_minute',
    'Average token generation rate (tokens/minute)',
    ['model'],
    help='Smoothed throughput metric'
)


# ============================================================================
# Model Performance Metrics
# ============================================================================

model_loading_duration_seconds = Gauge(
    'ryot_model_loading_duration_seconds',
    'Time to load LLM model in seconds',
    ['model'],
    help='Model cold-start loading time'
)

model_cache_hit_ratio = Gauge(
    'ryot_model_cache_hit_ratio',
    'Model cache hit rate (0.0 to 1.0)',
    ['model'],
    help='KV cache effectiveness for inference'
)

batch_size = Histogram(
    'ryot_batch_size',
    'Batch size for inference requests',
    ['model'],
    buckets=(1, 2, 4, 8, 16, 32, 64, 128),
    help='Number of requests processed together'
)

batches_processed_total = Counter(
    'ryot_batches_processed_total',
    'Total batches processed',
    ['model'],
    help='Number of batches completed'
)


# ============================================================================
# Resource Utilization Metrics
# ============================================================================

gpu_memory_usage_bytes = Gauge(
    'ryot_gpu_memory_usage_bytes',
    'GPU memory usage in bytes',
    ['model', 'gpu_id'],
    help='Current GPU memory consumption'
)

gpu_memory_reserved_bytes = Gauge(
    'ryot_gpu_memory_reserved_bytes',
    'GPU memory reserved in bytes',
    ['model', 'gpu_id'],
    help='Total memory allocated to model (vs. actually used)'
)

gpu_utilization_percent = Gauge(
    'ryot_gpu_utilization_percent',
    'GPU utilization percentage (0-100)',
    ['model', 'gpu_id'],
    help='GPU compute utilization'
)

memory_efficiency_ratio = Gauge(
    'ryot_memory_efficiency_ratio',
    'Memory efficiency (actual/reserved)',
    ['model'],
    help='How efficiently model uses allocated memory'
)


# ============================================================================
# Error and Quality Metrics
# ============================================================================

inference_errors_total = Counter(
    'ryot_inference_errors_total',
    'Total inference errors',
    ['model', 'error_type'],  # error_type: oom, timeout, invalid_input, cuda_error, etc.
    help='Error counts by type'
)

inference_retries_total = Counter(
    'ryot_inference_retries_total',
    'Total inference retries',
    ['model'],
    help='Number of retry attempts'
)

inference_queue_size = Gauge(
    'ryot_inference_queue_size',
    'Current inference queue size',
    ['model'],
    help='Number of requests waiting for processing'
)

inference_queue_wait_seconds = Histogram(
    'ryot_inference_queue_wait_seconds',
    'Time spent waiting in queue',
    ['model'],
    buckets=(0, 0.1, 0.5, 1.0, 5.0, 10.0, 30.0, 60.0),
    help='Queue latency before processing begins'
)


# ============================================================================
# System Information
# ============================================================================

system_info = Info(
    'ryot_system',
    'Ryot LLM system information',
)

model_info = Info(
    'ryot_model',
    'Information about loaded LLM models',
)


# ============================================================================
# Decorators for Inference Tracking
# ============================================================================

def track_inference_request(model: str = 'default'):
    """
    Decorator to track LLM inference requests
    
    Usage:
        @track_inference_request(model='gpt4')
        async def generate_completion(prompt: str):
            pass
    """
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            status = "success"
            token_count = 0
            
            try:
                result = await func(*args, **kwargs)
                
                # Extract token count if available
                if hasattr(result, 'token_count'):
                    token_count = result.token_count
                    tokens_generated_total.labels(model=model).inc(token_count)
                    tokens_per_request.labels(model=model).observe(token_count)
                
                if hasattr(result, 'ttft_seconds'):
                    inference_ttft_seconds.labels(model=model).observe(result.ttft_seconds)
                
                return result
            except TimeoutError:
                status = "timeout"
                logger.warning(f"Ryot inference timeout for model {model}")
                raise
            except RuntimeError as e:
                if 'out of memory' in str(e).lower():
                    status = "oom"
                    inference_errors_total.labels(model=model, error_type='oom').inc()
                else:
                    status = "error"
                    inference_errors_total.labels(model=model, error_type='runtime_error').inc()
                logger.error(f"Ryot inference error: {e}")
                raise
            except Exception as e:
                status = "error"
                inference_errors_total.labels(model=model, error_type='unknown').inc()
                logger.error(f"Ryot inference error: {e}")
                raise
            finally:
                duration = time.time() - start_time
                inference_requests_total.labels(model=model, status=status).inc()
                inference_request_duration_seconds.labels(model=model).observe(duration)
        
        return wrapper
    return decorator


def track_token_generation(model: str = 'default'):
    """
    Decorator to track token generation rates
    
    Usage:
        @track_token_generation(model='gpt4')
        async def generate_tokens(prompt: str):
            pass
    """
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            try:
                result = await func(*args, **kwargs)
                
                if hasattr(result, 'token_count'):
                    token_count = result.token_count
                    tokens_generated_total.labels(model=model).inc(token_count)
                
                if hasattr(result, 'duration_seconds'):
                    tps = result.token_count / result.duration_seconds if result.duration_seconds > 0 else 0
                    tokens_per_second.labels(model=model).set(tps)
                
                return result
            except Exception as e:
                logger.error(f"Token generation tracking error: {e}")
                raise
        
        return wrapper
    return decorator


# ============================================================================
# Helper Functions
# ============================================================================

def update_gpu_metrics(model: str, gpu_id: int, used_bytes: int, reserved_bytes: int, utilization: int):
    """Update GPU resource metrics"""
    gpu_memory_usage_bytes.labels(model=model, gpu_id=str(gpu_id)).set(used_bytes)
    gpu_memory_reserved_bytes.labels(model=model, gpu_id=str(gpu_id)).set(reserved_bytes)
    gpu_utilization_percent.labels(model=model, gpu_id=str(gpu_id)).set(utilization)
    
    if reserved_bytes > 0:
        efficiency = used_bytes / reserved_bytes
        memory_efficiency_ratio.labels(model=model).set(efficiency)


def record_inference_error(model: str, error_type: str):
    """Record inference error"""
    inference_errors_total.labels(model=model, error_type=error_type).inc()


def update_queue_metrics(model: str, queue_size: int):
    """Update queue depth metrics"""
    inference_queue_size.labels(model=model).set(queue_size)


def record_queue_wait(model: str, wait_seconds: float):
    """Record time spent in queue"""
    inference_queue_wait_seconds.labels(model=model).observe(wait_seconds)


def update_token_throughput(model: str, tokens_per_sec: float):
    """Update real-time token generation rate"""
    tokens_per_second.labels(model=model).set(tokens_per_sec)


def record_batch_processed(model: str, batch_size: int):
    """Record batch processing"""
    batch_size.labels(model=model).observe(batch_size)
    batches_processed_total.labels(model=model).inc()


def record_model_load_time(model: str, load_time_seconds: float):
    """Record model loading time"""
    model_loading_duration_seconds.labels(model=model).set(load_time_seconds)


def update_cache_metrics(model: str, hit_ratio: float):
    """Update KV cache effectiveness"""
    model_cache_hit_ratio.labels(model=model).set(hit_ratio)


# ============================================================================
# Context Managers
# ============================================================================

class InferenceContext:
    """Context manager for tracking inference requests"""
    
    def __init__(self, model: str = 'default'):
        self.model = model
        self.start_time = None
        self.token_count = 0
        self.status = "success"
    
    def __enter__(self):
        self.start_time = time.time()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        duration = time.time() - self.start_time
        
        if exc_type is not None:
            if exc_type == TimeoutError:
                self.status = "timeout"
            elif exc_type == RuntimeError and 'out of memory' in str(exc_val).lower():
                self.status = "oom"
                inference_errors_total.labels(model=self.model, error_type='oom').inc()
            else:
                self.status = "error"
                inference_errors_total.labels(model=self.model, error_type=exc_type.__name__).inc()
        
        inference_requests_total.labels(model=self.model, status=self.status).inc()
        inference_request_duration_seconds.labels(model=self.model).observe(duration)
        
        return False  # Don't suppress exceptions
    
    def set_token_count(self, count: int):
        """Record token count during request"""
        self.token_count = count
        tokens_generated_total.labels(model=self.model).inc(count)
        tokens_per_request.labels(model=self.model).observe(count)
    
    def set_ttft(self, ttft_seconds: float):
        """Record time to first token"""
        inference_ttft_seconds.labels(model=self.model).observe(ttft_seconds)
    
    def record_inter_token_latency(self, latency_seconds: float):
        """Record latency between tokens"""
        inference_inter_token_latency_seconds.labels(model=self.model).observe(latency_seconds)

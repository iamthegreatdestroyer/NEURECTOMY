# ============================================================================
# NEURECTOMY Prometheus Alert Rules
# ============================================================================

groups:
  # ==========================================================================
  # ML Service Alerts
  # ==========================================================================
  - name: ml-service
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(neurectomy_http_requests_total{status="error"}[5m]))
            /
            sum(rate(neurectomy_http_requests_total[5m]))
          ) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

      # High latency
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, 
            sum(rate(neurectomy_http_request_duration_seconds_bucket[5m])) by (le, endpoint)
          ) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency detected on {{ $labels.endpoint }}"
          description: "P95 latency is {{ $value | humanizeDuration }}"

      # Service down
      - alert: MLServiceDown
        expr: up{job="ml-service"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "ML Service is down"
          description: "ML Service has been unreachable for more than 1 minute"

  # ==========================================================================
  # Training Alerts
  # ==========================================================================
  - name: training
    rules:
      # Training job stuck
      - alert: TrainingJobStuck
        expr: |
          neurectomy_training_jobs_active > 0
          and on()
          increase(neurectomy_training_epoch_current[30m]) == 0
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Training job appears stuck"
          description: "No epoch progress in the last 30 minutes"

      # Too many failed training jobs
      - alert: HighTrainingFailureRate
        expr: |
          sum(rate(neurectomy_training_jobs_total{status="failed"}[1h]))
          /
          sum(rate(neurectomy_training_jobs_total[1h])) > 0.3
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "High training job failure rate"
          description: "{{ $value | humanizePercentage }} of training jobs failing"

      # Training loss not decreasing
      - alert: TrainingLossNotDecreasing
        expr: |
          delta(neurectomy_training_loss[1h]) > 0
        for: 2h
        labels:
          severity: info
        annotations:
          summary: "Training loss not decreasing for job {{ $labels.job_id }}"
          description: "Loss has increased over the last hour"

  # ==========================================================================
  # Resource Alerts
  # ==========================================================================
  - name: resources
    rules:
      # High GPU utilization
      - alert: HighGPUUtilization
        expr: neurectomy_gpu_utilization_percent > 95
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "High GPU utilization on GPU {{ $labels.gpu_id }}"
          description: "GPU utilization is {{ $value }}%"

      # GPU memory pressure
      - alert: GPUMemoryPressure
        expr: |
          neurectomy_gpu_memory_used_bytes 
          / 
          (neurectomy_gpu_memory_used_bytes + neurectomy_gpu_memory_free_bytes) > 0.9
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High GPU memory usage on GPU {{ $labels.gpu_id }}"
          description: "GPU memory usage is {{ $value | humanizePercentage }}"

      # Model cache too large
      - alert: ModelCacheTooLarge
        expr: neurectomy_model_cache_size_bytes > 10e9
        for: 5m
        labels:
          severity: info
        annotations:
          summary: "Model cache exceeds 10GB"
          description: "Cache size is {{ $value | humanize1024 }}"

  # ==========================================================================
  # Security Alerts
  # ==========================================================================
  - name: security
    rules:
      # High authentication failure rate
      - alert: HighAuthFailureRate
        expr: |
          sum(rate(neurectomy_auth_attempts_total{status="failed"}[5m]))
          /
          sum(rate(neurectomy_auth_attempts_total[5m])) > 0.2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High authentication failure rate"
          description: "{{ $value | humanizePercentage }} of auth attempts failing"

      # Possible brute force attack
      - alert: PossibleBruteForce
        expr: sum(rate(neurectomy_auth_attempts_total{status="failed"}[1m])) > 10
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Possible brute force attack detected"
          description: "{{ $value }} failed auth attempts per second"

      # High rate limit hits
      - alert: HighRateLimitHits
        expr: sum(rate(neurectomy_rate_limit_hits_total[5m])) > 1
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "High rate limit hits on {{ $labels.endpoint }}"
          description: "{{ $value }} rate limit hits per second"

  # ==========================================================================
  # Analytics Alerts
  # ==========================================================================
  - name: analytics
    rules:
      # Anomaly detected
      - alert: AnomalyDetected
        expr: |
          sum(increase(neurectomy_anomalies_detected_total{severity="high"}[5m])) > 0
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "High severity anomaly detected"
          description: "{{ $value }} anomalies detected in {{ $labels.metric_type }}"
  # ==========================================================================
  # SLO - Ryot (LLM Inference Service)
  # SLO: TTFT <50ms (99.9%), Error Rate <1% (99.9%)
  # Error Budget: 0.1% = 43.2 minutes/month
  # ==========================================================================
  - name: slo-ryot
    interval: 30s
    rules:
      # Ryot TTFT SLO - 30 day sliding window
      - record: slo:ryot:ttft:window_ratio
        expr: |
          (
            sum(increase(ryot_time_to_first_token_seconds_bucket{le="0.05"}[30d]))
            /
            sum(increase(ryot_time_to_first_token_seconds_count[30d]))
          )

      # Ryot Error Rate SLO - 30 day sliding window
      - record: slo:ryot:error_rate:window_ratio
        expr: |
          1 - (
            sum(increase(ryot_errors_total[30d]))
            /
            sum(increase(ryot_requests_total[30d]))
          )

      # Fast Burn Rate (1h = 30 days of budget)
      - alert: RyotSLOFastBurnRate
        expr: |
          (
            (
              sum(increase(ryot_time_to_first_token_seconds_bucket{le="0.05"}[1h]))
              /
              sum(increase(ryot_time_to_first_token_seconds_count[1h]))
            ) < 0.999
            or
            (
              1 - (
                sum(increase(ryot_errors_total[1h]))
                /
                sum(increase(ryot_requests_total[1h]))
              )
            ) < 0.999
          )
        for: 15m
        labels:
          severity: critical
          component: "ryot"
          slo_burn_type: "fast"
        annotations:
          summary: "Ryot SLO fast burn rate detected"
          description: "Error budget burn rate is >30x normal (1h of budget consumed in 2m)"

      # Slow Burn Rate (6h = 30 days of budget)
      - alert: RyotSLOSlowBurnRate
        expr: |
          (
            (
              sum(increase(ryot_time_to_first_token_seconds_bucket{le="0.05"}[6h]))
              /
              sum(increase(ryot_time_to_first_token_seconds_count[6h]))
            ) < 0.999
            or
            (
              1 - (
                sum(increase(ryot_errors_total[6h]))
                /
                sum(increase(ryot_requests_total[6h]))
              )
            ) < 0.999
          )
        for: 1h
        labels:
          severity: warning
          component: "ryot"
          slo_burn_type: "slow"
        annotations:
          summary: "Ryot SLO slow burn rate detected"
          description: "Error budget burn rate is >5x normal (1h of budget consumed in 12m)"

      # Critical SLO Breach
      - alert: CriticalSLOViolationRyot
        expr: |
          (
            (
              sum(increase(ryot_time_to_first_token_seconds_bucket{le="0.05"}[5m]))
              /
              sum(increase(ryot_time_to_first_token_seconds_count[5m]))
            ) < 0.99
            or
            (
              1 - (
                sum(increase(ryot_errors_total[5m]))
                /
                sum(increase(ryot_requests_total[5m]))
              )
            ) < 0.99
          )
        for: 1m
        labels:
          severity: critical
          component: "ryot"
          alert_type: "slo_breach"
        annotations:
          summary: "Ryot SLO violation - immediate action required"
          description: "SLO targets breached: TTFT or Error Rate"

  # ==========================================================================
  # SLO - ΣLANG (Compression Service)
  # SLO: Ratio >5x (99.5%), Success Rate >99% (99.5%)
  # Error Budget: 0.5% = 216 minutes/month
  # ==========================================================================
  - name: slo-sigma-lang
    interval: 30s
    rules:
      # ΣLANG Compression Ratio SLO
      - record: slo:sigmalang:ratio:window_ratio
        expr: |
          (
            sum(increase(sigmalang_compression_ratio[30d]))
            /
            sum(increase(sigmalang_compressions_total[30d]))
          ) >= 5.0

      # ΣLANG Success Rate SLO
      - record: slo:sigmalang:success:window_ratio
        expr: |
          1 - (
            sum(increase(sigmalang_failures_total[30d]))
            /
            sum(increase(sigmalang_operations_total[30d]))
          )

      # Fast Burn Rate
      - alert: SigmaLangSLOFastBurnRate
        expr: |
          (
            (
              sum(increase(sigmalang_compression_ratio[1h]))
              /
              sum(increase(sigmalang_compressions_total[1h]))
            ) < 5.0
            or
            (
              1 - (
                sum(increase(sigmalang_failures_total[1h]))
                /
                sum(increase(sigmalang_operations_total[1h]))
              )
            ) < 0.99
          )
        for: 15m
        labels:
          severity: critical
          component: "sigma-lang"
          slo_burn_type: "fast"
        annotations:
          summary: "ΣLANG SLO fast burn rate detected"
          description: "Compression ratio or success rate degrading rapidly"

      # Slow Burn Rate
      - alert: SigmaLangSLOSlowBurnRate
        expr: |
          (
            (
              sum(increase(sigmalang_compression_ratio[6h]))
              /
              sum(increase(sigmalang_compressions_total[6h]))
            ) < 5.0
            or
            (
              1 - (
                sum(increase(sigmalang_failures_total[6h]))
                /
                sum(increase(sigmalang_operations_total[6h]))
              )
            ) < 0.99
          )
        for: 1h
        labels:
          severity: warning
          component: "sigma-lang"
          slo_burn_type: "slow"
        annotations:
          summary: "ΣLANG SLO slow burn rate detected"
          description: "Compression efficiency trending downward"

  # ==========================================================================
  # SLO - ΣVAULT (Storage Service)
  # SLO: Availability >99.99% (99.99%), Latency <100ms p95 (99.9%)
  # Error Budget: 0.01% = 4.32 minutes/month (availability)
  # ==========================================================================
  - name: slo-sigma-vault
    interval: 30s
    rules:
      # ΣVAULT Availability SLO
      - record: slo:sigmavault:availability:window_ratio
        expr: |
          1 - (
            sum(increase(sigmavault_failed_requests_total[30d]))
            /
            sum(increase(sigmavault_requests_total[30d]))
          )

      # ΣVAULT Latency SLO
      - record: slo:sigmavault:latency:window_ratio
        expr: |
          (
            sum(increase(sigmavault_request_duration_seconds_bucket{le="0.1"}[30d]))
            /
            sum(increase(sigmavault_request_duration_seconds_count[30d]))
          )

      # Fast Burn Rate
      - alert: SigmaVaultSLOFastBurnRate
        expr: |
          (
            (
              1 - (
                sum(increase(sigmavault_failed_requests_total[1h]))
                /
                sum(increase(sigmavault_requests_total[1h]))
              )
            ) < 0.9999
            or
            (
              sum(increase(sigmavault_request_duration_seconds_bucket{le="0.1"}[1h]))
              /
              sum(increase(sigmavault_request_duration_seconds_count[1h]))
            ) < 0.999
          )
        for: 15m
        labels:
          severity: critical
          component: "sigma-vault"
          slo_burn_type: "fast"
        annotations:
          summary: "ΣVAULT SLO fast burn rate detected"
          description: "Storage availability or latency degrading rapidly"

      # Slow Burn Rate
      - alert: SigmaVaultSLOSlowBurnRate
        expr: |
          (
            (
              1 - (
                sum(increase(sigmavault_failed_requests_total[6h]))
                /
                sum(increase(sigmavault_requests_total[6h]))
              )
            ) < 0.9999
            or
            (
              sum(increase(sigmavault_request_duration_seconds_bucket{le="0.1"}[6h]))
              /
              sum(increase(sigmavault_request_duration_seconds_count[6h]))
            ) < 0.999
          )
        for: 1h
        labels:
          severity: warning
          component: "sigma-vault"
          slo_burn_type: "slow"
        annotations:
          summary: "ΣVAULT SLO slow burn rate detected"
          description: "Storage performance trending downward"

  # ==========================================================================
  # SLO - Agent Collective Health
  # SLO: Health Score >95% (99%), Throughput Stable >±10% (99%)
  # ==========================================================================
  - name: slo-agent-collective
    interval: 30s
    rules:
      # Agent Health Score SLO
      - record: slo:agent:health:ratio
        expr: |
          avg(increase(neurectomy_agent_health_score[30d]))

      # Throughput Stability SLO
      - record: slo:agent:throughput:variance
        expr: |
          stddev(rate(neurectomy_agent_tasks_completed_total[1m])) / avg(rate(neurectomy_agent_tasks_completed_total[1m]))

      # Health Score Alert
      - alert: AgentCollectiveHealthDegradation
        expr: |
          avg(rate(neurectomy_agent_health_score[5m])) < 0.95
        for: 5m
        labels:
          severity: warning
          component: "agent-collective"
        annotations:
          summary: "Agent Collective health score degraded"
          description: "Health score is {{ $value | humanizePercentage }} (target: >95%)"

      # Throughput Instability Alert
      - alert: AgentCollectiveThroughputInstability
        expr: |
          (stddev(rate(neurectomy_agent_tasks_completed_total[5m])) / avg(rate(neurectomy_agent_tasks_completed_total[5m]))) > 0.1
        for: 10m
        labels:
          severity: warning
          component: "agent-collective"
        annotations:
          summary: "Agent Collective throughput unstable"
          description: "Throughput variance is {{ $value | humanizePercentage }} (threshold: ±10%)"
